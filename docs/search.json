[
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "Python QMD Example",
    "section": "",
    "text": "This is an example of how to use Python code in a R Markdown document. We will use the reticulate package to run Python code in this R Markdown document.\nTo a void issues when first using this template, or cloning to a new device. The python code chunks have been commented out so it doesn‚Äôt trigger the jupyter notebook. Triggering the jupyter notebook will require a python environment, and the required jupyter notebook libraries. Downloading this libraries can be a hassle if not using python. When using Python you will have to follow the steps to make sure your project is properly set up to run python code in R-studio.\n#```{python} # Import common libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt"
  },
  {
    "objectID": "python.html#python-example-with-common-libraries",
    "href": "python.html#python-example-with-common-libraries",
    "title": "Python QMD Example",
    "section": "",
    "text": "This is an example of how to use Python code in a R Markdown document. We will use the reticulate package to run Python code in this R Markdown document.\nTo a void issues when first using this template, or cloning to a new device. The python code chunks have been commented out so it doesn‚Äôt trigger the jupyter notebook. Triggering the jupyter notebook will require a python environment, and the required jupyter notebook libraries. Downloading this libraries can be a hassle if not using python. When using Python you will have to follow the steps to make sure your project is properly set up to run python code in R-studio.\n#```{python} # Import common libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Week Introdcutions",
    "section": "",
    "text": "Week 1 Introduction\n\n\n\n\n Back to top",
    "crumbs": [
      "Week Introductions"
    ]
  },
  {
    "objectID": "intro/week2.html",
    "href": "intro/week2.html",
    "title": "Project title here",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Week Introductions",
      "Week 2 Intro"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Warehousing",
    "section": "",
    "text": "Week 1 introduction notes"
  },
  {
    "objectID": "index.html#quick-summary-week-1-concepts",
    "href": "index.html#quick-summary-week-1-concepts",
    "title": "Data Warehousing",
    "section": "üìù Quick Summary: Week 1 Concepts",
    "text": "üìù Quick Summary: Week 1 Concepts\n\nüîπ Data Mart\n\nSubset of a data warehouse.\nDepartment-specific (e.g., sales, finance).\nFast, small-scale, quick deployment.\nUses star schema.\nRetains summarized data, short-term.\nEasier to manage (&lt; 100GB).\nCan be dependent, independent, or hybrid.\n\n\n\nüîπ Data Warehouse\n\nCentral data repository for entire org.\nIntegrates multiple subject areas.\nLarge-scale, long-term historical storage.\nUses complex schemas (star, snowflake, etc).\nSupports deep analytics and BI.\nStores detailed normalized/denormalized data.\nSubject-oriented and time-variant.\n\n\n\nüîπ Reliability\n\nSystem works correctly even if parts fail.\nNeeds testing, error recovery, monitoring.\n\n\n\nüîπ Scalability\n\nHandles growth in data/users/load.\nMeasured using response time, load metrics.\n\n\n\nüîπ Maintainability\n\nEasy to fix, extend, and operate.\nKey principles: operability, simplicity, evolvability.\nWeek 1 introduction\nView Project\nView Details\nView Details"
  },
  {
    "objectID": "intro/week1.html",
    "href": "intro/week1.html",
    "title": "Week 1 Introduction",
    "section": "",
    "text": "Learn the difference between a data mart and a data warehouse.\n\nüéØ Learning Outcomes\n\nUnderstand the scope of a data mart\n\nUnderstand the scope of a data warehouse\n\nLearn how data marts may improve the development telemetry of data warehouses\n\nLearn how data marts may improve the quality of data warehouse design\n\n\nüìò Reading Assignment\nChapter 1 ‚Äî Reliable, Scalable, and Maintainable Applications\n\n\n\n\n\n\n\n\n\nFigure¬†1: Data Warehouse & Data Marts\n\n\n\nUse Cases: Data Marts vs.¬†Data Warehouses\n\n\n\n\n\n\n\n\n\nUse Case\nScenario\nRole\nType\n\n\n\n\nMarketing Campaign Analysis\nAnalyze effectiveness of recent campaigns\nMarketing data mart provides quick insights on engagement and conversion rates\nData Mart\n\n\nSales Performance Tracking\nSales team needs real-time performance data\nSales data mart offers focused access to salesperson metrics and regional trends\nData Mart\n\n\nFinancial Planning & Analysis\nFinance needs detailed data for budgeting and forecasting\nFinancial data mart delivers department-specific info for planning and analysis\nData Mart\n\n\nEnterprise-wide Reporting\nCEO needs comprehensive company performance report\nData warehouse integrates data across departments for full organizational view\nData Warehouse\n\n\nHistorical Trend Analysis\nAnalyze 10-year trends for strategic decisions\nData warehouse stores and retrieves historical data for long-term trend analysis\nData Warehouse\n\n\nComplex BI Queries\nAnalysts run complex queries across multiple sources\nData warehouse supports deep analytics through optimized query performance\nData Warehouse\n\n\n\nTable: Data warehouse vs Data Mart\n\n\n\n\n\n\n\nData Warehouse\nData Mart\n\n\n\n\nA data warehouse is used to store data from numerous subject areas.\nA data mart carries data related to a department, such as HR, marketing, finance, etc.\n\n\nIt acts as a central data repository for a company.\nIt is a logical subsection of a data warehouse for particular departmental applications.\n\n\nDesigned using star, snowflake, galaxy, or fact constellation schema.\nData marts use a star schema for designing tables.\n\n\nTricky to design and use due to its large size (more than 100GB).\nComparatively more manageable due to its small size (less than 100GB).\n\n\nSupports the decision-making process company-wide.\nDesigned for specific user groups or corporate departments.\n\n\nStores detailed information in denormalized or normalized form.\nHolds highly denormalized data in summarized form.\n\n\nHas large dimensions and integrates data from many sources.\nSmaller dimensions to integrate data from fewer sources.\n\n\nSubject-oriented and time-variant with long-term data retention.\nUsed for specific business areas with shorter-term data retention.\n\n\n\n\nUnderstand the scope of a data mart\n\nUnderstand the scope of a data warehouse\n\nLearn how data marts may improve the development telemetry of data warehouses\n\nLearn how data marts may improve the quality of data warehouse design\n\n\n\nA data mart is a specialized subset of a data warehouse. It focuses on a specific business function, department, or user group within an organization. Data marts are designed to provide different departments with access to relevant data so they can independently explore and extract insights specific to their unique requirements, ultimately fostering more informed and targeted decision-making. It draws information from only a particular source or a subset of a data warehouse.\nKey aspects of a data mart‚Äôs scope include:\n\nSubject-Oriented: Data marts are often aligned with a specific business area like marketing, finance, or sales.\nSmaller Scale: Data marts are comparatively more manageable due to their small size (less than 100GB). This allows for faster query performance as the data is focused.\nFaster Deployment: Data marts are quicker to deploy compared to large-scale data warehouses, offering a more agile solution for specific business units needing rapid access to analytics.\nCost-Effective (for smaller projects): Implementing data marts can be more cost-effective for smaller-scale projects as you build targeted marts fulfilling specific needs rather than a massive infrastructure.\nUse Star Schema: Data marts typically use a star schema for designing tables, which is generally simpler to understand and query.\nShorter Data Retention: Data marts are often used for particular areas related to a business and may retain data for a shorter duration compared to a data warehouse.\nDecentralized System: A data mart is often considered a decentralized system compared to a data warehouse.\n\nThere are different types of data marts:\n\nDependent Data Marts: Built using an existing data warehouse, taking a top-down approach.\nIndependent Data Marts: Standalone entities not directly connected to the data warehouse, built directly from external sources in a bottom-up approach. These can become hard to manage as businesses expand due to separate ETL tools and logic.\nHybrid Data Marts: Combine elements of both dependent and independent data marts, leveraging the coherence of a central warehouse while incorporating specific external data.\n\n\n\n\nA data warehouse is a centralized data repository that stores large volumes of structured and often unstructured data from various sources within an organization. It acts as a central data repository for a company, providing a single source of truth (SSOT) for existing and historical data for analysis and data-driven decision-making.\nKey aspects of a data warehouse‚Äôs scope include:\n\nEnterprise-Wide Analysis: Designed for comprehensive and in-depth analysis across various departments and functions, providing a unified view for comprehensive reporting and analysis.\nData Integration: Crucial for integrating and cleansing diverse data sources, ensuring data consistency and reliability across the organization through robust ETL (Extract, Transform, Load) processes.\nScalability: Offers scalability to handle massive amounts of data efficiently (more than 100GB, potentially terabytes or petabytes) and support complex querying and reporting needs.\nComplex Schema Design: Can be designed using various schemas like star, snowflake, galaxy, or fact constellation schema, although star schema is widely used. These schemas can be more complex than those used in data marts.\nLonger Data Retention: Data warehouses store detailed information in denormalized or normalized form and are time-variant, with data existing for a longer duration to facilitate historical trend analysis.\nSubject-Oriented: Focuses on providing information concerning a subject rather than a business‚Äôs daily operations, emphasizing business intelligence (BI).\nUnified and Integrated: Unifies and integrates data from different databases in a collectively suitable manner, incorporating data from diverse sources with consistent classification, layout, and coding.\nNon-volatile: Does not remove primary data when new information is loaded, allowing only data reading and intermittent refreshing for a complete and updated picture.\n\n\n\n\nDevelopment telemetry in the context of data warehouses can be understood as the detailed and clear monitoring of the data warehouse development process, including performance metrics, data quality metrics, error rates during ETL, and the efficiency of query execution for specific business needs. Just as telemetry is essential for tracking a rocket, it‚Äôs crucial for understanding the health and progress of a data warehouse project.\nData marts can improve the development telemetry of data warehouses in several ways:\n\nFocused Monitoring: Since data marts cater to specific business needs, monitoring efforts can be more targeted. Instead of broadly monitoring the entire data warehouse, development teams can focus on the performance and data quality within each data mart in relation to its intended use. This allows for more granular and insightful telemetry.\nEarly Issue Detection: Developing and deploying data marts incrementally (as in the Kimball methodology) allows for earlier detection of issues in the data integration and transformation processes related to specific business areas. Problems identified in a smaller data mart are generally easier to diagnose and resolve than issues that might surface only after a large, complex data warehouse is fully built. The focused telemetry on a data mart can provide early warning signals.\nFaster Feedback Loops: The quicker deployment and smaller scope of data marts facilitate faster feedback loops from the business users. Their usage patterns and reported issues within the data mart serve as valuable telemetry, indicating whether the data is meeting their analytical needs and highlighting areas for improvement in the underlying data warehouse design and ETL processes.\nPerformance Measurement for Specific Use Cases: Telemetry gathered from data marts directly reflects the performance of the data warehouse for specific analytical use cases. Metrics like query response times within a marketing data mart provide direct insights into how well the data warehouse supports marketing campaign analysis.\nValidation of Data Transformations: The data within a data mart undergoes specific transformations relevant to its business function. Monitoring the data quality within the data mart (e.g., through validation rules defined during ETL) provides telemetry on the effectiveness and correctness of these transformations, which can then inform and improve the broader data transformation processes within the data warehouse.\n\n\n\n\nData marts can contribute to a higher quality data warehouse design through several mechanisms:\n\nIterative and Agile Approach: The Kimball approach, which advocates building data warehouses through subject-specific data marts, promotes an iterative and agile development process. By starting with smaller, focused data marts, designers can gather requirements and validate their design decisions incrementally. This iterative feedback loop helps refine the data model and ETL processes of the underlying data warehouse, leading to a more robust and user-centric design.\nUnderstanding Business Needs: Data marts are driven by specific business requirements. Designing and implementing data marts forces data warehouse architects to have a deep understanding of the analytical needs of different business units. This granular understanding of how data will be used leads to a more relevant and effective overall data warehouse design.\nTesting Ground for Data Models and ETL: Data marts can serve as a testing ground for different data modeling techniques (like star schema) and ETL strategies on a smaller scale. Lessons learned from the design and implementation of data marts, including performance bottlenecks and data quality issues, can be applied to improve the design of the larger data warehouse.\nConformed Dimensions and Data Quality: The Kimball methodology emphasizes the use of conformed dimensions across different data marts. This ensures that a single data item (like ‚Äòcustomer‚Äô or ‚Äòproduct‚Äô) is used consistently across the organization. The process of conforming dimensions for data marts contributes to better data quality and consistency within the entire data warehouse.\nPrioritization Based on Business Value: The development of data marts allows for prioritization based on immediate business value. By focusing on delivering data marts that address critical analytical needs first, organizations can ensure that the data warehouse development effort is aligned with business priorities and that the core data models and ETL processes are validated early on. This helps in building a data warehouse that truly meets the organization‚Äôs needs.\n\n\n\nUnderstand the meaning of reliability\n\nUnderstand the meaning of scalability\n\nUnderstand the meaning of maintainability\n\n\n\n\n\nSystem works correctly even when things go wrong.\nKey aspects:\n\nPerforms as expected.\nTolerates user mistakes/unexpected use.\nHandles expected load and data size.\nPrevents unauthorized access.\n\nFault ‚â† Failure: Fault = component issue; Failure = service interruption.\nFault tolerance is essential.\nTechniques:\n\nTest all levels (unit, integration, manual).\nFast recovery from human errors (e.g.¬†rollbacks).\nClear monitoring (metrics, error rates).\nGood management and training (not covered deeply).\n\n\n\n\n\n\nAbility to handle increased load (data, traffic, complexity).\nAsk: ‚ÄúHow do we scale if load grows?‚Äù\nDefine load using load parameters (e.g.¬†RPS, user count).\nMeasure with response time percentiles.\nScaling often needs re-architecting as load increases.\nUse general-purpose building blocks in scalable patterns.\n\n\n\n\n\nEasy for devs/ops to work on long-term (fix, run, adapt, evolve).\nThree principles:\n\nOperability: Easy system operation (monitoring, automation, docs, sane defaults).\nSimplicity: Easy to understand; reduce complexity via good abstractions.\nEvolvability: Easy to change/adapt system in future (also called extensibility).\n\n\n\n\n\n\nReliability: System works correctly despite faults.\nScalability: System handles growth efficiently.\nMaintainability: System is easy to operate, understand, and modify.",
    "crumbs": [
      "Week Introductions",
      "Week 1 Intro"
    ]
  },
  {
    "objectID": "intro/week1.html#reading-outcomes",
    "href": "intro/week1.html#reading-outcomes",
    "title": "Week 1 Introduction",
    "section": "",
    "text": "Figure¬†1: Data Warehouse & Data Marts\n\n\n\nUse Cases: Data Marts vs.¬†Data Warehouses\n\n\n\n\n\n\n\n\n\nUse Case\nScenario\nRole\nType\n\n\n\n\nMarketing Campaign Analysis\nAnalyze effectiveness of recent campaigns\nMarketing data mart provides quick insights on engagement and conversion rates\nData Mart\n\n\nSales Performance Tracking\nSales team needs real-time performance data\nSales data mart offers focused access to salesperson metrics and regional trends\nData Mart\n\n\nFinancial Planning & Analysis\nFinance needs detailed data for budgeting and forecasting\nFinancial data mart delivers department-specific info for planning and analysis\nData Mart\n\n\nEnterprise-wide Reporting\nCEO needs comprehensive company performance report\nData warehouse integrates data across departments for full organizational view\nData Warehouse\n\n\nHistorical Trend Analysis\nAnalyze 10-year trends for strategic decisions\nData warehouse stores and retrieves historical data for long-term trend analysis\nData Warehouse\n\n\nComplex BI Queries\nAnalysts run complex queries across multiple sources\nData warehouse supports deep analytics through optimized query performance\nData Warehouse\n\n\n\nTable: Data warehouse vs Data Mart\n\n\n\n\n\n\n\nData Warehouse\nData Mart\n\n\n\n\nA data warehouse is used to store data from numerous subject areas.\nA data mart carries data related to a department, such as HR, marketing, finance, etc.\n\n\nIt acts as a central data repository for a company.\nIt is a logical subsection of a data warehouse for particular departmental applications.\n\n\nDesigned using star, snowflake, galaxy, or fact constellation schema.\nData marts use a star schema for designing tables.\n\n\nTricky to design and use due to its large size (more than 100GB).\nComparatively more manageable due to its small size (less than 100GB).\n\n\nSupports the decision-making process company-wide.\nDesigned for specific user groups or corporate departments.\n\n\nStores detailed information in denormalized or normalized form.\nHolds highly denormalized data in summarized form.\n\n\nHas large dimensions and integrates data from many sources.\nSmaller dimensions to integrate data from fewer sources.\n\n\nSubject-oriented and time-variant with long-term data retention.\nUsed for specific business areas with shorter-term data retention.\n\n\n\n\nUnderstand the scope of a data mart\n\nUnderstand the scope of a data warehouse\n\nLearn how data marts may improve the development telemetry of data warehouses\n\nLearn how data marts may improve the quality of data warehouse design\n\n\n\nA data mart is a specialized subset of a data warehouse. It focuses on a specific business function, department, or user group within an organization. Data marts are designed to provide different departments with access to relevant data so they can independently explore and extract insights specific to their unique requirements, ultimately fostering more informed and targeted decision-making. It draws information from only a particular source or a subset of a data warehouse.\nKey aspects of a data mart‚Äôs scope include:\n\nSubject-Oriented: Data marts are often aligned with a specific business area like marketing, finance, or sales.\nSmaller Scale: Data marts are comparatively more manageable due to their small size (less than 100GB). This allows for faster query performance as the data is focused.\nFaster Deployment: Data marts are quicker to deploy compared to large-scale data warehouses, offering a more agile solution for specific business units needing rapid access to analytics.\nCost-Effective (for smaller projects): Implementing data marts can be more cost-effective for smaller-scale projects as you build targeted marts fulfilling specific needs rather than a massive infrastructure.\nUse Star Schema: Data marts typically use a star schema for designing tables, which is generally simpler to understand and query.\nShorter Data Retention: Data marts are often used for particular areas related to a business and may retain data for a shorter duration compared to a data warehouse.\nDecentralized System: A data mart is often considered a decentralized system compared to a data warehouse.\n\nThere are different types of data marts:\n\nDependent Data Marts: Built using an existing data warehouse, taking a top-down approach.\nIndependent Data Marts: Standalone entities not directly connected to the data warehouse, built directly from external sources in a bottom-up approach. These can become hard to manage as businesses expand due to separate ETL tools and logic.\nHybrid Data Marts: Combine elements of both dependent and independent data marts, leveraging the coherence of a central warehouse while incorporating specific external data.\n\n\n\n\nA data warehouse is a centralized data repository that stores large volumes of structured and often unstructured data from various sources within an organization. It acts as a central data repository for a company, providing a single source of truth (SSOT) for existing and historical data for analysis and data-driven decision-making.\nKey aspects of a data warehouse‚Äôs scope include:\n\nEnterprise-Wide Analysis: Designed for comprehensive and in-depth analysis across various departments and functions, providing a unified view for comprehensive reporting and analysis.\nData Integration: Crucial for integrating and cleansing diverse data sources, ensuring data consistency and reliability across the organization through robust ETL (Extract, Transform, Load) processes.\nScalability: Offers scalability to handle massive amounts of data efficiently (more than 100GB, potentially terabytes or petabytes) and support complex querying and reporting needs.\nComplex Schema Design: Can be designed using various schemas like star, snowflake, galaxy, or fact constellation schema, although star schema is widely used. These schemas can be more complex than those used in data marts.\nLonger Data Retention: Data warehouses store detailed information in denormalized or normalized form and are time-variant, with data existing for a longer duration to facilitate historical trend analysis.\nSubject-Oriented: Focuses on providing information concerning a subject rather than a business‚Äôs daily operations, emphasizing business intelligence (BI).\nUnified and Integrated: Unifies and integrates data from different databases in a collectively suitable manner, incorporating data from diverse sources with consistent classification, layout, and coding.\nNon-volatile: Does not remove primary data when new information is loaded, allowing only data reading and intermittent refreshing for a complete and updated picture.\n\n\n\n\nDevelopment telemetry in the context of data warehouses can be understood as the detailed and clear monitoring of the data warehouse development process, including performance metrics, data quality metrics, error rates during ETL, and the efficiency of query execution for specific business needs. Just as telemetry is essential for tracking a rocket, it‚Äôs crucial for understanding the health and progress of a data warehouse project.\nData marts can improve the development telemetry of data warehouses in several ways:\n\nFocused Monitoring: Since data marts cater to specific business needs, monitoring efforts can be more targeted. Instead of broadly monitoring the entire data warehouse, development teams can focus on the performance and data quality within each data mart in relation to its intended use. This allows for more granular and insightful telemetry.\nEarly Issue Detection: Developing and deploying data marts incrementally (as in the Kimball methodology) allows for earlier detection of issues in the data integration and transformation processes related to specific business areas. Problems identified in a smaller data mart are generally easier to diagnose and resolve than issues that might surface only after a large, complex data warehouse is fully built. The focused telemetry on a data mart can provide early warning signals.\nFaster Feedback Loops: The quicker deployment and smaller scope of data marts facilitate faster feedback loops from the business users. Their usage patterns and reported issues within the data mart serve as valuable telemetry, indicating whether the data is meeting their analytical needs and highlighting areas for improvement in the underlying data warehouse design and ETL processes.\nPerformance Measurement for Specific Use Cases: Telemetry gathered from data marts directly reflects the performance of the data warehouse for specific analytical use cases. Metrics like query response times within a marketing data mart provide direct insights into how well the data warehouse supports marketing campaign analysis.\nValidation of Data Transformations: The data within a data mart undergoes specific transformations relevant to its business function. Monitoring the data quality within the data mart (e.g., through validation rules defined during ETL) provides telemetry on the effectiveness and correctness of these transformations, which can then inform and improve the broader data transformation processes within the data warehouse.\n\n\n\n\nData marts can contribute to a higher quality data warehouse design through several mechanisms:\n\nIterative and Agile Approach: The Kimball approach, which advocates building data warehouses through subject-specific data marts, promotes an iterative and agile development process. By starting with smaller, focused data marts, designers can gather requirements and validate their design decisions incrementally. This iterative feedback loop helps refine the data model and ETL processes of the underlying data warehouse, leading to a more robust and user-centric design.\nUnderstanding Business Needs: Data marts are driven by specific business requirements. Designing and implementing data marts forces data warehouse architects to have a deep understanding of the analytical needs of different business units. This granular understanding of how data will be used leads to a more relevant and effective overall data warehouse design.\nTesting Ground for Data Models and ETL: Data marts can serve as a testing ground for different data modeling techniques (like star schema) and ETL strategies on a smaller scale. Lessons learned from the design and implementation of data marts, including performance bottlenecks and data quality issues, can be applied to improve the design of the larger data warehouse.\nConformed Dimensions and Data Quality: The Kimball methodology emphasizes the use of conformed dimensions across different data marts. This ensures that a single data item (like ‚Äòcustomer‚Äô or ‚Äòproduct‚Äô) is used consistently across the organization. The process of conforming dimensions for data marts contributes to better data quality and consistency within the entire data warehouse.\nPrioritization Based on Business Value: The development of data marts allows for prioritization based on immediate business value. By focusing on delivering data marts that address critical analytical needs first, organizations can ensure that the data warehouse development effort is aligned with business priorities and that the core data models and ETL processes are validated early on. This helps in building a data warehouse that truly meets the organization‚Äôs needs.\n\n\n\nUnderstand the meaning of reliability\n\nUnderstand the meaning of scalability\n\nUnderstand the meaning of maintainability\n\n\n\n\n\nSystem works correctly even when things go wrong.\nKey aspects:\n\nPerforms as expected.\nTolerates user mistakes/unexpected use.\nHandles expected load and data size.\nPrevents unauthorized access.\n\nFault ‚â† Failure: Fault = component issue; Failure = service interruption.\nFault tolerance is essential.\nTechniques:\n\nTest all levels (unit, integration, manual).\nFast recovery from human errors (e.g.¬†rollbacks).\nClear monitoring (metrics, error rates).\nGood management and training (not covered deeply).\n\n\n\n\n\n\nAbility to handle increased load (data, traffic, complexity).\nAsk: ‚ÄúHow do we scale if load grows?‚Äù\nDefine load using load parameters (e.g.¬†RPS, user count).\nMeasure with response time percentiles.\nScaling often needs re-architecting as load increases.\nUse general-purpose building blocks in scalable patterns.\n\n\n\n\n\nEasy for devs/ops to work on long-term (fix, run, adapt, evolve).\nThree principles:\n\nOperability: Easy system operation (monitoring, automation, docs, sane defaults).\nSimplicity: Easy to understand; reduce complexity via good abstractions.\nEvolvability: Easy to change/adapt system in future (also called extensibility).\n\n\n\n\n\n\nReliability: System works correctly despite faults.\nScalability: System handles growth efficiently.\nMaintainability: System is easy to operate, understand, and modify.",
    "crumbs": [
      "Week Introductions",
      "Week 1 Intro"
    ]
  },
  {
    "objectID": "intro/week1.html#reliability-1",
    "href": "intro/week1.html#reliability-1",
    "title": "Week 1 Introduction",
    "section": "Reliability",
    "text": "Reliability\nReliability means that a system should continue to work correctly even when things go wrong. This implies several aspects:\n\nThe application performs the function that the user expected.\nIt can tolerate users making mistakes or using the software in unexpected ways.\nIts performance is good enough for the required use case, under the expected load and data volume.\nThe system prevents any unauthorized access and abuse.\n\nChapter 1 clarifies that a fault, which is when a component deviates from its specification, is different from a failure, which is when the entire system stops providing the required service. Since it‚Äôs impossible to eliminate faults entirely, reliable systems are built with fault-tolerance mechanisms to prevent faults from causing failures.\nAchieving reliability involves various techniques:\n\nThorough testing at all levels (unit, integration, manual) is crucial, especially for corner cases.\nAllowing quick and easy recovery from human errors minimizes the impact of failures, for example, through fast rollbacks and gradual code rollouts.\nSetting up detailed and clear monitoring (telemetry), including performance metrics and error rates, provides early warnings and helps diagnose issues.\nImplementing good management practices and training (though beyond the scope of the book).",
    "crumbs": [
      "Week Introductions",
      "Week 1 Intro"
    ]
  },
  {
    "objectID": "intro/week1.html#scalability-1",
    "href": "intro/week1.html#scalability-1",
    "title": "Week 1 Introduction",
    "section": "Scalability",
    "text": "Scalability\nScalability describes a system‚Äôs ability to cope with increased load. It‚Äôs not a simple label but rather a consideration of how the system handles growth in areas like data volume, traffic volume, or complexity. Discussing scalability involves questions like, ‚ÄúIf the system grows in a particular way, what are our options for coping with the growth?‚Äù and ‚ÄúHow can we add computing resources to handle the additional load?‚Äù.\nTo understand scalability, it‚Äôs necessary to first describe the current load on the system using load parameters relevant to the system‚Äôs architecture (e.g., requests per second, read/write ratio, number of active users). Performance can then be measured using metrics like response time percentiles.\nMaintaining good performance under increased load often requires rethinking the system‚Äôs architecture as load increases by orders of magnitude. Scalable architectures are typically built from general-purpose building blocks arranged in familiar patterns.",
    "crumbs": [
      "Week Introductions",
      "Week 1 Intro"
    ]
  },
  {
    "objectID": "intro/week1.html#maintainability-1",
    "href": "intro/week1.html#maintainability-1",
    "title": "Week 1 Introduction",
    "section": "Maintainability",
    "text": "Maintainability\nMaintainability focuses on making the system easy to work on over time for various people (engineering and operations). This includes tasks like fixing bugs, keeping the system operational, adapting it to new platforms and use cases, repaying technical debt, and adding new features.\nChapter 1 breaks down maintainability into three key design principles:\n\nOperability: Making it easy for operations teams to keep the system running smoothly. This involves things like providing visibility into runtime behavior through good monitoring, supporting automation and integration with standard tools, avoiding dependency on individual machines, providing good documentation, and offering sensible defaults with the option to override them.\nSimplicity: Making the system easy for new engineers to understand by removing as much complexity as possible. This doesn‚Äôt refer to the user interface but to the internal design of the system. Complexity can manifest in various ways, leading to increased risk of bugs and difficulty in maintenance. Good abstractions can help manage complexity by extracting parts of the system into well-defined, reusable components.\nEvolvability: Making it easy for engineers to make changes to the system in the future, adapting it to changing requirements and unanticipated use cases. This is closely linked to simplicity and good abstractions, as simple and understandable systems are generally easier to modify. Evolvability is also known as extensibility, modifiability, or plasticity.\n\nIn summary, reliability ensures the system works correctly despite faults, scalability ensures it can handle increased load effectively, and maintainability ensures it can be easily operated, understood, and evolved over time. These three principles are fundamental to the design of data-intensive applications.\nSources\n\nComparison of data marts vs data warehouses with a visual summary chart\n\nOverview of functional differences between data marts and data warehouses\n\nSummary of leading data warehouse platforms (cloud and on-premise)\n\nGartner‚Äôs assessment of cloud data warehouse vendors\n\nGartner‚Äôs evaluation of business intelligence tools layered over data warehouses\n\nG2‚Äôs grid of business intelligence platforms for descriptive analytics",
    "crumbs": [
      "Week Introductions",
      "Week 1 Intro"
    ]
  },
  {
    "objectID": "intro/week3.html",
    "href": "intro/week3.html",
    "title": "Week 3 Introduction",
    "section": "",
    "text": "Learn how ETL moves data from native stores to a data mart or warehouse\n\nüéØ Learning Outcomes\n\nLearn what extract, transform, and load (ETL Figure¬†1) does.\n\nLearn what type of SQL queries are required to extract data.\n\nLearn what type of transformation is required to change the data from Online Transactional Processing (OLTP) format to Online Analytical Processing (OLAP). Figure¬†2 F\n\nLearn what type of loading mechanisms exist for major databases.\n\n\nüìò Reading Assignment\nChapter 3 ‚Äî Storage and Retrieval\n\n\n\n\nLearn the meaning of hash indexes, SSTables, LSM-Trees, and B-Trees.\n\nLearn the meaning of star and snowflake schemas for analytics.\n\nLearn the meaning of column-oriented storage.\n\n\n\n\nExtract, Transform, Load (ETL) is a three-phase computing process used to combine and synthesize raw data from multiple data sources into a data warehouse, data lake, or other data store. It is central to building OLAP systems from OLTP sources.\nETL may include extended phases such as: - Cycle initiation - Reference data setup - Staging and validation - Audit reporting - Publishing and archiving\n\n\n\nCollects data from structured and semi-structured sources:\n\nRelational DBs(Figure¬†7 Figure¬†6), flat files (Figure¬†8), XML, JSON, APIs, web scraping\n\nPerforms validation checks on extracted data\nMay use:\n\nSQL SELECT queries\nDirect path extract\nBulk unload for performance\n\n\n\n\n\n\nApplies transformations to prepare data for analytical storage:\n\nData cleansing\nColumn filtering/renaming\nValue translation/encoding\nDerived metrics (e.g., revenue = qty √ó price)\nJoins, aggregations, and deduplication\nSurrogate key generation\nTranspose, split/disaggregate columns\nData validation\n\nConverts OLTP ‚Üí OLAP:\n\nNormalized ‚Üí Denormalized (Star or Snowflake schema)\nBuild fact tables (measures + FKs)\nBuild dimension tables (contextual attributes)\nAdd surrogate keys (support SCDs)\nAggregate for summary analytics\n\n\n\n\n\n\nLoads data into final analytical target:\n\nData warehouse, lake, mart\nMay overwrite or append (based on historical tracking)\n\nOptimizations:\n\nUse bulk load and parallel loading (if partitioned)\nDisable constraints/triggers during load\nDrop/rebuild indexes before/after\nValidate data before loading\nGenerate surrogate IDs outside of DB (ETL layer)\n\n\n\n\n\n\n\n\nFigure¬†1\n\n\n\n\n\n\n\n\n\nSQL is standard for querying relational DB (Figure¬†6) sources\nCommon: SELECT (with filtering, joins)\nBest practices:\n\nUse direct path extract or bulk unload for large volume\nAvoid query-heavy extraction on OLTP systems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAspect\nOLTP Systems\nOLAP Systems\n\n\n\n\nQuery Volume\nHigh volume, short reads/writes\nLow volume, long, aggregate queries\n\n\nAccess Pattern\nPoint queries by key\nScans across many rows\n\n\nSchema\nNormalized (3NF)\nDenormalized (star/snowflake)\n\n\nPurpose\nOperational/transactional\nAnalytical, historical insights\n\n\n\n\n\n\n\n\n\nFigure¬†2: OLAP and OLTP Systems connection to Users\n\n\n\n\n\n\nData cleansing (ensure quality)\nData conformance (unify formats)\nAggregation (totals, averages)\nSurrogate keys (independent warehouse IDs)\nHandle SCDs (track historical changes)\nSchema restructuring into:\n\nFact tables (measures + FKs)\nDimension tables (context: who/what/when/etc.)\n\n\n\n\n\n\n\n\nTarget systems:\n\nTraditional RDBMS\nData warehouses (e.g., Redshift, BigQuery, Synapse, Snowflake)\nData lakes\n\n\n\n\n\nUse bulk loading tools\nUse parallel bulk load for partitioned or index-free tables\nPre-load validation in ETL layer\nDisable integrity checks temporarily\nGenerate IDs in ETL logic\nDrop/rebuild indexes around load window\nAlternative to API-based inserts/updates/deletes:\n\nSeparate by operation type\nUse bulk for inserts, SQL API for updates/deletes\nConsider replication for DB-to-DB transfers\n\n\n\n\n\n\n\n\n\n\nMost common in OLTP RDBMS\nPage-based, sorted by key Figure¬†11\nFast lookups and range queries\nO(log n) performance\n\n\n\n\n\nImmutable, sorted key-value segments Figure¬†12\nUsed in LSM-trees and Lucene\nMerged in background for optimization Figure¬†14\n\n\n\n\n\nStack of SSTables with background compaction Figure¬†13\nWrite-optimized: sequential disk writes\nEfficient for range queries even at scale\nUsed in LevelDB, RocksDB, Cassandra, HBase, Lucene\n\n\n\n\n\nConstant time key lookups (if in-memory)\nNot efficient for range queries\nUsed in relational DBs alongside B+ trees\n\n\n\n\n\n\n\nFigure¬†3: Storing a log of key-value pairs in a CSV-like format, indexed with an in memory hash map.\n\n\n\n\n\n\n\n\n\n\n\nCentral fact table with numerical measures Figure¬†9\nLinked to multiple dimension tables\nDimensions: who, what, where, when, how\nExample: fact_sales joins to dim_store, dim_date, dim_product\nSimple structure ‚Üí preferred for analyst queries\n\n\n\n\n\n\n\nFigure¬†4: Star Schema - Source: Matt Tanguay Linkedin\n\n\n\n\n\n\n\nNormalized version of star schema\nDimension tables may be broken into sub-dimensions\nExample: dim_product ‚Üí dim_category, dim_brand\nMore joins, but saves space and increases structure\n\n\n\n\n\n\n\nFigure¬†5: Snowflake Schema - Source: Matt Tanguay Linkedin\n\n\n\n\nOLAP cube metadata is usually generated from star/snowflake schemas.\n\n\n\n\n\n\n\nStores data by column, not by row Figure¬†10\nOptimal for analytical queries that:\n\nAccess few columns across many rows\nUse aggregate functions\n\n\n\n\n\nRead fewer bytes ‚Üí faster query execution\nImproved compression (RLE, dictionary, bitmap)\nEnables vectorized processing\n\n\n\n\n\nWrites are slower/harder to manage\nUsed in analytical DBs:\n\nRedshift, BigQuery, Snowflake, Parquet, DuckDB\n\n\n\nIdeal when disk I/O and large-scale scanning dominate workload",
    "crumbs": [
      "Week Introductions",
      "Week 3 Intro"
    ]
  },
  {
    "objectID": "intro/week3.html#reading-outcomes",
    "href": "intro/week3.html#reading-outcomes",
    "title": "Week 3 Introduction",
    "section": "",
    "text": "Learn the meaning of hash indexes, SSTables, LSM-Trees, and B-Trees.\n\nLearn the meaning of star and snowflake schemas for analytics.\n\nLearn the meaning of column-oriented storage.",
    "crumbs": [
      "Week Introductions",
      "Week 3 Intro"
    ]
  },
  {
    "objectID": "intro/week3.html#etl-process-and-data-movement",
    "href": "intro/week3.html#etl-process-and-data-movement",
    "title": "Week 3 Introduction",
    "section": "",
    "text": "Extract, Transform, Load (ETL) is a three-phase computing process used to combine and synthesize raw data from multiple data sources into a data warehouse, data lake, or other data store. It is central to building OLAP systems from OLTP sources.\nETL may include extended phases such as: - Cycle initiation - Reference data setup - Staging and validation - Audit reporting - Publishing and archiving\n\n\n\nCollects data from structured and semi-structured sources:\n\nRelational DBs(Figure¬†7 Figure¬†6), flat files (Figure¬†8), XML, JSON, APIs, web scraping\n\nPerforms validation checks on extracted data\nMay use:\n\nSQL SELECT queries\nDirect path extract\nBulk unload for performance\n\n\n\n\n\n\nApplies transformations to prepare data for analytical storage:\n\nData cleansing\nColumn filtering/renaming\nValue translation/encoding\nDerived metrics (e.g., revenue = qty √ó price)\nJoins, aggregations, and deduplication\nSurrogate key generation\nTranspose, split/disaggregate columns\nData validation\n\nConverts OLTP ‚Üí OLAP:\n\nNormalized ‚Üí Denormalized (Star or Snowflake schema)\nBuild fact tables (measures + FKs)\nBuild dimension tables (contextual attributes)\nAdd surrogate keys (support SCDs)\nAggregate for summary analytics\n\n\n\n\n\n\nLoads data into final analytical target:\n\nData warehouse, lake, mart\nMay overwrite or append (based on historical tracking)\n\nOptimizations:\n\nUse bulk load and parallel loading (if partitioned)\nDisable constraints/triggers during load\nDrop/rebuild indexes before/after\nValidate data before loading\nGenerate surrogate IDs outside of DB (ETL layer)\n\n\n\n\n\n\n\n\nFigure¬†1",
    "crumbs": [
      "Week Introductions",
      "Week 3 Intro"
    ]
  },
  {
    "objectID": "intro/week3.html#sql-queries-for-extraction",
    "href": "intro/week3.html#sql-queries-for-extraction",
    "title": "Week 3 Introduction",
    "section": "",
    "text": "SQL is standard for querying relational DB (Figure¬†6) sources\nCommon: SELECT (with filtering, joins)\nBest practices:\n\nUse direct path extract or bulk unload for large volume\nAvoid query-heavy extraction on OLTP systems",
    "crumbs": [
      "Week Introductions",
      "Week 3 Intro"
    ]
  },
  {
    "objectID": "intro/week3.html#transformation-from-oltp-format-to-olap-format",
    "href": "intro/week3.html#transformation-from-oltp-format-to-olap-format",
    "title": "Week 3 Introduction",
    "section": "",
    "text": "Aspect\nOLTP Systems\nOLAP Systems\n\n\n\n\nQuery Volume\nHigh volume, short reads/writes\nLow volume, long, aggregate queries\n\n\nAccess Pattern\nPoint queries by key\nScans across many rows\n\n\nSchema\nNormalized (3NF)\nDenormalized (star/snowflake)\n\n\nPurpose\nOperational/transactional\nAnalytical, historical insights\n\n\n\n\n\n\n\n\n\nFigure¬†2: OLAP and OLTP Systems connection to Users\n\n\n\n\n\n\nData cleansing (ensure quality)\nData conformance (unify formats)\nAggregation (totals, averages)\nSurrogate keys (independent warehouse IDs)\nHandle SCDs (track historical changes)\nSchema restructuring into:\n\nFact tables (measures + FKs)\nDimension tables (context: who/what/when/etc.)",
    "crumbs": [
      "Week Introductions",
      "Week 3 Intro"
    ]
  },
  {
    "objectID": "intro/week3.html#loading-mechanisms-for-major-databases",
    "href": "intro/week3.html#loading-mechanisms-for-major-databases",
    "title": "Week 3 Introduction",
    "section": "",
    "text": "Target systems:\n\nTraditional RDBMS\nData warehouses (e.g., Redshift, BigQuery, Synapse, Snowflake)\nData lakes\n\n\n\n\n\nUse bulk loading tools\nUse parallel bulk load for partitioned or index-free tables\nPre-load validation in ETL layer\nDisable integrity checks temporarily\nGenerate IDs in ETL logic\nDrop/rebuild indexes around load window\nAlternative to API-based inserts/updates/deletes:\n\nSeparate by operation type\nUse bulk for inserts, SQL API for updates/deletes\nConsider replication for DB-to-DB transfers",
    "crumbs": [
      "Week Introductions",
      "Week 3 Intro"
    ]
  },
  {
    "objectID": "intro/week3.html#indexing-structures",
    "href": "intro/week3.html#indexing-structures",
    "title": "Week 3 Introduction",
    "section": "",
    "text": "Most common in OLTP RDBMS\nPage-based, sorted by key Figure¬†11\nFast lookups and range queries\nO(log n) performance\n\n\n\n\n\nImmutable, sorted key-value segments Figure¬†12\nUsed in LSM-trees and Lucene\nMerged in background for optimization Figure¬†14\n\n\n\n\n\nStack of SSTables with background compaction Figure¬†13\nWrite-optimized: sequential disk writes\nEfficient for range queries even at scale\nUsed in LevelDB, RocksDB, Cassandra, HBase, Lucene\n\n\n\n\n\nConstant time key lookups (if in-memory)\nNot efficient for range queries\nUsed in relational DBs alongside B+ trees\n\n\n\n\n\n\n\nFigure¬†3: Storing a log of key-value pairs in a CSV-like format, indexed with an in memory hash map.",
    "crumbs": [
      "Week Introductions",
      "Week 3 Intro"
    ]
  },
  {
    "objectID": "intro/week3.html#star-and-snowflake-schemas-for-analytics",
    "href": "intro/week3.html#star-and-snowflake-schemas-for-analytics",
    "title": "Week 3 Introduction",
    "section": "",
    "text": "Central fact table with numerical measures Figure¬†9\nLinked to multiple dimension tables\nDimensions: who, what, where, when, how\nExample: fact_sales joins to dim_store, dim_date, dim_product\nSimple structure ‚Üí preferred for analyst queries\n\n\n\n\n\n\n\nFigure¬†4: Star Schema - Source: Matt Tanguay Linkedin\n\n\n\n\n\n\n\nNormalized version of star schema\nDimension tables may be broken into sub-dimensions\nExample: dim_product ‚Üí dim_category, dim_brand\nMore joins, but saves space and increases structure\n\n\n\n\n\n\n\nFigure¬†5: Snowflake Schema - Source: Matt Tanguay Linkedin\n\n\n\n\nOLAP cube metadata is usually generated from star/snowflake schemas.",
    "crumbs": [
      "Week Introductions",
      "Week 3 Intro"
    ]
  },
  {
    "objectID": "intro/week3.html#column-oriented-storage",
    "href": "intro/week3.html#column-oriented-storage",
    "title": "Week 3 Introduction",
    "section": "",
    "text": "Stores data by column, not by row Figure¬†10\nOptimal for analytical queries that:\n\nAccess few columns across many rows\nUse aggregate functions\n\n\n\n\n\nRead fewer bytes ‚Üí faster query execution\nImproved compression (RLE, dictionary, bitmap)\nEnables vectorized processing\n\n\n\n\n\nWrites are slower/harder to manage\nUsed in analytical DBs:\n\nRedshift, BigQuery, Snowflake, Parquet, DuckDB\n\n\n\nIdeal when disk I/O and large-scale scanning dominate workload",
    "crumbs": [
      "Week Introductions",
      "Week 3 Intro"
    ]
  },
  {
    "objectID": "projects/star_project_one/oltp_to_star_schema.html",
    "href": "projects/star_project_one/oltp_to_star_schema.html",
    "title": "OLTP to Dimensional Modeling",
    "section": "",
    "text": "Overview\nTask: Create a data warehouse dimensional model by using the MySQL Workbench software.\nPurpose: Learn how to re-model data from an Online Transaction Processing (OLTP) system to an Online Analytical Processing (OLAP) system or data warehouse using a Star Schema as the modeling solution."
  },
  {
    "objectID": "projects/star_project_one/oltp_to_star_schema.html#instructions",
    "href": "projects/star_project_one/oltp_to_star_schema.html#instructions",
    "title": "OLTP to Dimensional Modeling",
    "section": "Instructions",
    "text": "Instructions\n\nGroup Collaboration: Complete this lab with your W03 Presentation: ETL Processes group.\n\nYou have just started a new position at a company. Your manager expects you to learn the distinction between a data mart and a data warehouse, and has encouraged you to consult online resources for clarity."
  },
  {
    "objectID": "projects/star_project_one/oltp_to_star_schema.html#review-preparation-articles",
    "href": "projects/star_project_one/oltp_to_star_schema.html#review-preparation-articles",
    "title": "OLTP to Dimensional Modeling",
    "section": "1. Review preparation articles:",
    "text": "1. Review preparation articles:\n\nReverse Engineering\n\nForward Engineering\n\nCreating a New ERD Model\n\nBasic ERD Modeling\nAdding a Table\n\nCreating a Foreign Key"
  },
  {
    "objectID": "projects/star_project_one/oltp_to_star_schema.html#install-mysql-workbench",
    "href": "projects/star_project_one/oltp_to_star_schema.html#install-mysql-workbench",
    "title": "OLTP to Dimensional Modeling",
    "section": "2. Install MySQL Workbench:",
    "text": "2. Install MySQL Workbench:\nInstall MySQL Workbench using MySQL Workbench Installation Guide"
  },
  {
    "objectID": "projects/star_project_one/oltp_to_star_schema.html#download-the-starting-model",
    "href": "projects/star_project_one/oltp_to_star_schema.html#download-the-starting-model",
    "title": "OLTP to Dimensional Modeling",
    "section": "3. Download the starting model:",
    "text": "3. Download the starting model:\nYou can click on this ITM425 Model to download your starting MySQL Workbench file."
  },
  {
    "objectID": "projects/star_project_one/oltp_to_star_schema.html#error-checking",
    "href": "projects/star_project_one/oltp_to_star_schema.html#error-checking",
    "title": "OLTP to Dimensional Modeling",
    "section": "4. Error Checking:",
    "text": "4. Error Checking:\n\nThe starting point for the lab is an Online Transaction Processing (OLTP) Video Database ERD. You will find it attached to the lab. Your task is to forward engineer with the model to identify any shortfalls with the design and fix them. You can check for errors by running the forward engineering script, noting the errors and fixing them in the base data model."
  },
  {
    "objectID": "projects/star_project_one/oltp_to_star_schema.html#design-star-schema",
    "href": "projects/star_project_one/oltp_to_star_schema.html#design-star-schema",
    "title": "OLTP to Dimensional Modeling",
    "section": "5. Design Star Schema:",
    "text": "5. Design Star Schema:\n\nAfter you have fixed all the errors, you will have a new Video Model. You use that model to design a star schema with dimension and fact tables. You should build the fact table with transactions; and you should build dimension tables that let you filter for customers, locale, item, and member account."
  },
  {
    "objectID": "projects/star_project_one/oltp_to_star_schema.html#star-schema-requirements",
    "href": "projects/star_project_one/oltp_to_star_schema.html#star-schema-requirements",
    "title": "OLTP to Dimensional Modeling",
    "section": "6. Star Schema Requirements",
    "text": "6. Star Schema Requirements\nYou start with over 60 tables in the completed relational model of the Video Store. You may MySQL Workbench locally, or in a Docker instance to convert the OLTP ERD into a Kimball Star Schema with one fact table and at least three dimension tables. The tables should at least model the following:\n\nOne dimension table should include the item table, inclusive of item ratings, rating agencies, genre, and item types from the common_lookup table data.\nOne dimension table should include the member account, contact information including address and telephone information.\nOne dimension table should include the date. This table would have one record per day and would have attributes about that day to allow the end user to filter that data. See the Kimball Group definition of a standard date dimension.\nOne fact table should include data from the rental and rental_item tables with the rental_item data setting the granularity of the fact table."
  },
  {
    "objectID": "projects/star_project_one/oltp_to_star_schema.html#star-schema-requirements-1",
    "href": "projects/star_project_one/oltp_to_star_schema.html#star-schema-requirements-1",
    "title": "OLTP to Dimensional Modeling",
    "section": "Star Schema requirements",
    "text": "Star Schema requirements\n\n\nFact table requirements\n\nTransaction Fact Tables1\n\n\nAtomic transaction grain\n\nEach row represents a measurement event at a specific point in space and time.\n\nForeign keys for each associated dimension\n\nLinks to dimension tables such as customer, product, time, etc.\n\nMeasured numeric facts\n\nMust be consistent with the transaction grain (e.g., quantity, price, revenue).\n\nOptional precise time stamps\n\nFor finer temporal analysis beyond standard date keys.\n\nOptional degenerate dimension keys\n\ne.g., invoice numbers or transaction IDs stored directly in the fact table.\n\nDimensional richness\n\nHigh number of dimensions enables detailed slicing and dicing of the data.\n\nDensity can vary\n\nDense: frequent events\nSparse: events only when a transaction occurs\n\n\n\nnatural keys and Surrogate2 keys\nNulls3 in Fact Tables\n\n\nDimensional Table Requirements\n\nDimension Table Structure4\n\n\nSingle primary key column: Uniquely identifies each row in the dimension table. Used as a foreign key in associated fact tables.\nDenormalized, wide, flat structure: Contains many textual, low-cardinality attributes. Optimized for readability and query performance.\nVerbose descriptive attributes: 1. Prefer clear descriptions over operational codes or abbreviations. 2. Enhances usability in reports and dashboards.\nAttributes used for filtering and grouping: Primary targets for query constraints, group-by clauses, and BI tools.\nDimension attribute domain values: These are typically the labels shown in reports and visualizations.\n\n\n\nDimension Surrogate Keys5\n\n\nSingle primary key column: Each dimension table has one unique primary key.\nMust not use natural keys from operational systems: 1. Multiple dimension rows may exist for the same natural key (e.g., for tracking changes over time). 2. Natural keys may come from multiple source systems and can be inconsistent or poorly managed.\nUse surrogate keys instead: 1. Anonymous integer keys, assigned sequentially starting from 1. 2. Maintains control and consistency within the DW/BI system.\nAvoid composite keys (e.g., natural key + date): Simplifies joins and indexing.\nException: date dimension: Can use a meaningful, natural primary key (e.g., yyyymmdd) due to its stability and predictability.\n\n\n\nNatural, Durable, and Supernatural Keys6\n\n\nCreated by operational source systems: Natural keys reflect business rules and processes outside the control of the DW/BI system.\nProne to change: Example: an employee number may change if an employee is rehired, making the key unstable over time.\nNot suitable as a persistent identifier: Cannot guarantee durability or uniqueness across time and systems.\nMust be replaced with durable keys: Data warehouses should assign a new durable (supernatural) key, independent of business logic.\nDurable key format should be simple: Use sequential integers (starting from 1) to ensure consistency and manageability.\n\n\n\nNull Attributes in Dimensions7\n\nAvoid: Null values in dimension attributes\nUse instead: Descriptive placeholders like ‚ÄúUnknown‚Äù or ‚ÄúNot Applicable‚Äù\nWhy avoid nulls: Grouping and filtering on nulls is inconsistent across databases"
  },
  {
    "objectID": "projects/star_project_one/oltp_to_star_schema.html#footnotes",
    "href": "projects/star_project_one/oltp_to_star_schema.html#footnotes",
    "title": "OLTP to Dimensional Modeling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA row in a transaction fact table corresponds to a measurement event at a point in space and time. Atomic transaction grain fact tables are the most dimensional and expressive fact tables; this robust dimensionality enables the maximum slicingand dicing of transaction data. Transaction fact tables may be dense or sparse because rows exist only if measurements take place. These fact tables always contain a foreign key for each associated dimension, and optionally contain precise time stamps and degenerate dimension keys. The measured numeric facts must be consistent with the transaction grain.‚Ü©Ô∏é\nthe primary key is an auto-generated integer that has no meaning for the business entity being represented, but solely exists for the purpose of the relational database commonly referred to as a surrogate key.‚Ü©Ô∏é\nnulls must be avoided in the fact table‚Äôs foreign keys because these nulls would automatically cause a referential in tegrity violation. Rather than a null foreign key, the associated dimension table must have a default row (and surrogate key) representing the unknown or not applicable condition.‚Ü©Ô∏é\nEvery dimension table has a single primary key column. This primary key is embedded as a foreign key in any associated fact table where the dimension row‚Äôs descriptive context is exactly correct for that fact table row. Dimension tables are usually wide, flat denormalized tables with many low-cardinality text attributes. While operational codes and indicators can be treated as attributes, the most powerful dimension attributes are populated with verbose descriptions. Dimension table attributes are the primary target of constraints and grouping specifications from queries and BI applications. The descriptive labels on reports are typically dimension attribute domain values.‚Ü©Ô∏é\ndimension table is designed with one column serving as a unique primary key. This primary key cannot be the operational system‚Äôs natural key because there will be multiple dimension rows for that natural key when changes are tracked over time. In addition, natural keys for a dimension may be created by more than one source system, and these natural keys may be incompatible or poorly administered. The DW/BI system needs to claim control of the primary keys of all dimensions; rather than using explicit natural keys or natural keys with appended dates, you should create anonymous integer primary keys for every dimension. These dimension surrogate keys are simple integers, assigned in sequence, starting with the value 1, every time a new key is needed. The date dimension is exempt from the surrogate key rule; this highly predictable and stable dimension can use a more meaningful primary key. See the section ‚ÄúCalendar Date Dimensions.‚Äù‚Ü©Ô∏é\nNatural keys created by operational source systems are subject to business rules outside the control of the DW/BI system. For instance, an employee number (natural key) may be changed if the employee resigns and then is rehired. When the data warehouse wants to have a single key for that employee, a new durable key must be created that is persistent and does not change in this situation. This key is sometimes referred to as a durable supernatural key. The best durable keys have a format that is independent of the original business process and thus should be simple integers assigned in sequence beginning with 1. While multiple surrogate keys may be associated with an employee over time as their profile changes, the durable key never changes.‚Ü©Ô∏é\nNull-valued dimension attributes result when a given dimension row has not been fully populated, or when there are attributes that are not applicable to all the dimension‚Äôs rows. In both cases, we recommend substituting a descriptive string, such as Unknown or Not Applicable in place of the null value. Nulls in dimension attributes should be avoided because different databases handle grouping and constraining on nulls inconsistently.‚Ü©Ô∏é"
  }
]